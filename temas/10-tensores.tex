\documentclass[../VD.tex]{subfiles}

\externaldocument{../VD}

\begin{document}

\setcounter{chapter}{9}
\chapter{Espacios de formas y tensores sobre una variedad}\label{chap:dual} 

\section{Introducción}

En \textbf{Álgebra Lineal} conocemos la extensión natural del espacio dual de
las formas.

\begin{definition}
  Si una 1-forma del espacio vectorial \(V\) es una aplicación lineal
  \(V\to\RealSet\), entonces dados \(n\) espacios vectoriales
  \(V_{1},\dots,V_{n}\) y otro espacio vectorial \(W\), una \emph{aplicación
    multilineal} \(f\colon V_{1}\times\dots\times V_{n}\to W\), esto es, \(\forall
  \lambda,\mu\in\RealSet,\ \forall v_{i},v'_{i}\in V_{i},\ 1\leq i\leq n\):
  
  \[
    f(v_{1},\dots,\lambda v_{i}+\mu v'_{i},\dots,v_{n})=\lambda
    f(\dots,v_{i},\dots)+\mu f(\dots,v'_{i},\dots).
  \]

  Cuando \(V_{1}=\dots=V_{n},\ W=\RealSet\) decimos que \(f\) es una n-forma
  sobre \(V\).
\end{definition}

Ahora se plantea cómo extender las n-formas al espacio tangente de una variedad
\(M\). Ya vimos en el caso de las 1-formas que esa construcción se puede abordar
de varias maneras:

\begin{enumerate}
\item Crear el espacio cotangente haciendo la construcción punto a punto y
  después definiendo una estructura coherente en todo \(M\): para cada \(p\in
  M\) sea \(\DualT[p]{M}\) y se define \(\DualT{M}=\bigcup_{p\in
    M}\DualT[p]{M}\), dotándolo de una estructura de variedad diferenciable
  compatible con la de \(M\) mediante la proyección
  \(\pi^{*}\colon\DualT{M}\to M\ \omega\in\DualT[p]{M}\mapsto p\) y las 1-formas
  se definen como secciones diferenciables de esa proyección.

\item Definir el espacio cotangente globalmente sobre todo \(\Tangente{M}\) por
  medio de funciones diferenciables \(f\colon\Tangente{M}\to\RealSet\) que punto
  a punto son 1-formas, es decir, \(f\colon\Tangente[p]{M}\to\RealSet\) es
  lineal para todo \(p\in M\).

\item Usar \(\mathbb{X}(M)\) el \(\mathcal{F}(M)\)-módulo de los campos sobre
  \(M\) y reescribir \(\DualT{M}\) como el dual de \(\mathbb{X}(M)\) como
  \(\mathcal{F}(M)\)-módulo. Esto es, usar la identificación
  \(\DualT{M}\cong\text{Hom}(\mathbb{X}(M),\mathcal{F}(M))\), que lleva la forma
  \(\omega\colon M\to\Tangente{M}\) a la función \(\omega(X)(p)=X(\omega(p)),\
  \forall p\in M\).
\end{enumerate}

Veamos cómo extender estas construccione equivalentes del espacio cotangente de
\(M\) para obtener la noción de n-forma sobre la variedad \(M\).

Para ello recordemos la noción de producto tensorial que permite reinterpretar
las aplicaciones multilineales como 1-formas, pasando la complicación de las
aplicaciones multilineales al espacio vectorial.

\section{Producto Tensorial}

\begin{definition}[name=producto tensorial]
  Recordemos que el \emph{producto tensorial} de los espacios vectoriales
  \(V_{1},\dots,V_{n}\), \(V_{1}\bigotimes\dots\bigotimes V_{n}\), es el espacio
  vectorial obtenido por combinaciones lineales de los símbolos
  \(v_{1}\otimes\dots\otimes\ v_{n}\) sujetos a las relaciones:

  \begin{itemize}
  \item \(\lambda(v_{1}\otimes\dots\otimes
    v_{n})=v_{1}\otimes\dots\otimes \lambda v_{i}\otimes\dots\otimes
    v_{n},\ \forall i\)

  \item \(v_{1}\otimes\dots\otimes(v_{i}+v'_{i})\otimes\dots\otimes
    v_{n}=v_{1}\otimes\dots\otimes \lambda v_{i}\otimes\dots\otimes
    v_{n}+v_{1}\otimes\dots\otimes \lambda v'_{i}\otimes\dots\otimes
    v_{n},\ \forall i\)
  \end{itemize}

  Entonces \((V_{1}\bigotimes\dots\bigotimes
  V_{n})^{*}\cong\text{Multi}_{n}(V_{1},\dots,V_{n}),\ f\mapsto
  g_{f}(v_{1},\dots,v_{n})=f(v_{1}\otimes\dots\otimes\v_{n})\).
\end{definition}

\begin{remark}
  Además, se tiene que si \(B_{i}\) es una base de \(V_{i}\) entonces
  \(\{b_{k_{1}}\otimes\dots\otimes b_{k_{n}}\}\) es una base de
  \(V_{1}\bigotimes\dots\bigotimes V_{n}\) donde
  \((b_{k_{1}},\dots,b_{k_{n}})\in B_{1}\times\dots\times B_{n}\).
  Equivalentemente, las n-formas \(\delta_{k_{1},\dots,k_{n}}\) dadas por las
  extensiones multilineales de \(\delta_{k_{1},\dots,k_{n}}(b_{k_{1}}\dots
  b_{k_{n}})=1\) y \(\delta_{k_{1},\dots,k_{n}}(b_{k'_{1}}\dots b_{k'_{n}})=0\)
  si \((k'_{1},\dots,k'_{n})\neq(k_{1},\dots,k_{n})\) es una base del espacio de
  las aplicaciones multilineales \(V_{1}\times\dots\times V_{n}\to\RealSet\).
  
  Si \(V_{1}=\dots=V_{n}=V\) denotamos por \(\bigotimes_{n}V\) a
    \(\underbracket{V\bigotimes\dots\bigotimes V}_{n\text{ veces}}\).

  Así pues si Multi\(_{n}(V)\) es el espacio vectorial de las n-formas sobre
  \(V\) tenemos Multi\(_{n}(V)\cong(\bigotimes_{n}(V))^{*}\) y
  \(\dim{\text{Multi}_{n}(V)}=(\dim{V})^{n}\).

  Además, dadas \(\alpha\in(\bigotimes_{r}V)^{*}\) y
  \(\beta\in(\bigotimes_{s}V)^{*}\) se define
  \(\alpha\otimes\beta\in(\bigotimes_{r}V\bigotimes(\bigotimes_{s}V))^{*}\) por:
  \[
    \alpha\otimes\beta(v_{1}\otimes\dots\otimes v_{r}\otimes
    v_{r+1}\otimes\dots\otimes v_{r+s})=\alpha(v_{1}\otimes\dots\otimes
    v_{r})\cdot \beta(v_{r+1}\otimes\dots\otimes v_{r+s}). 
  \]

  Obsérvese que existe un isomorfismo
  \(\rho\colon\bigotimes_{r}V\cong(\bigotimes_{r}V)^{*}\) que lleva el producto
  tensorial de 1-formas \(\alpha_{1}\otimes\dots\otimes\alpha_{r}\) en la
  1-forma sobre \(\bigotimes_{r}V\) (o n-forma sobre \(V\)):
  \[
    \rho(\alpha_{1}\otimes\dots\otimes\alpha_{r})(v_{1}\otimes\dots\otimes
    v_{r})=\alpha_{1}(v_{1})\otimes\dots\otimes\alpha_{r}(v_{r})
  \]

  Nótese que si \(\{e_{i}\}\) es una base de \(V\), los elementos básicos de
  \(\bigotimes_{r}V^{*}\), \(e_{i_{1}}^{*}\otimes\dots\otimes e_{i_{r}}^{*}\)
  pasan por \(\rho\) a los duales \((e_{i_{1}}\otimes\dots\otimes
  e_{i_{n}})^{*}\) de la base \(\{e_{i_{1}}\otimes\dots\otimes e_{i_{r}}\}\) de
  \(\bigotimes_{r}V\).
\end{remark}

Así pues, las opciones (equivalentes) para generalizar el espacio cotangente son
las siguientes:

\begin{enumerate}
\item Tomar para cada \(p\in M\) Multi\(_{n}(\Tangente[p]{M})\) y definir
  Multi\(_{n}\Tangente{M}=\bigcup_{p\in M}\text{Multi}_{n}(\Tangente[p]{M})\).
  Si se quiere usar el producto tensorial entonces se toma
  \(\bigotimes_{n}\Tangente[p]{M}\) y \(\bigotimes_{n}\Tangente{M}=\bigcup_{p\in
    M}\bigotimes_{n}\Tangente[p]{M}\). Habrá que dotar a
  Multi\(_{n}(\Tangente{M})\cong\bigotimes_{n}\Tangente{M}\) de una estructura de
  variedad de dimensión \(n\dim{M}\), para la cual la proyección
  \(\pi^{*}\colon\bigotimes_{n}\Tangente{M}\to M\) es diferenciable y definir una
  n-forma sobre \(M\) como una sección de \(\pi\).
  
\item Definir una n-forma como una aplicación diferenciable
  \(f\colon\bigotimes_{n}\Tangente{M}\to\RealSet\) tal que para cada \(p\in M\)
  se restringe a una aplicación lineal
  \(f\colon\Tangente[p]{M}\otimes\dots\otimes\Tangente[p]{M}\to\RealSet\), o,
  equivalentemente multilineal
  \(f\colon\Tangente[p]{M}\times\dots\times\Tangente[p]{M}\to\RealSet\).

  Esta construcción es equivalente a definir primero el espacio tangente
  ''producto'' \((\Tangente{M})^{n}=\bigcup_{p\in M}(\Tangente[p]{M})^{n}\),
  donde
  \((\Tangente[p]{M})^{n}=\Tangente[p]{M}\times\dots\times\Tangente[p]{M}\) que
  resulta ser una variedad de dimensión \(n\dim{M}\). Ahora una n-forma sobre
  \(M\) será una aplicación diferenciable
  \(f\colon(\Tangente{M})^{n}\to\RealSet\) tal que para todo \(p\in M\) se
  restringe a una aplicación multilineal
  \(f\colon\Tangente[p]{M}\times\dots\times\Tangente[p]{M}\to\RealSet\).

\item Extender las nociones de aplicación multilineal y producto tensorial a
  módulos sobre un anillo \(A\). Entonces considerar las aplicaciones
  multilineales de \(\mathcal{F}(M)\)-módulos
  \(\mathbb{X}(M)\times\dots\times\mathbb{X}(M)\to\RealSet\) o bien las
  aplicaciones lineales de \(\mathcal{F}(M)\)-módulos
  \(\mathbb{X}(M)\bigotimes\dots\bigotimes\mathbb{X}(M)\to\mathcal{F}(M)\).
\end{enumerate}

\begin{lemma}
  Las 3 construcciones anteriores son equivalentes.
\end{lemma}

\begin{proof}
  Ahora vemos cómo pasar de una a otra construcción:
  \begin{itemize}
  \item \((1)\iff(2)\). Si \(s\colon M\to\text{Multi}_{n}(\Tangente{M})\) es una
    sección defino
    \(f_{s}\colon\Tangente[p]{M}\times\dots\times\Tangente[p]{M}\to\RealSet\) por
    \(f_{s}(v_{1},\dots,v_{n})=s(p)(v_{1},\dots,v_{n}),\
    v_{i}\in\Tangente[p]{M}\).
    Recíprocamente, dada \(f\colon\bigotimes_{n}\Tangente{M}\to\RealSet\) se
    define \(s_{f}(p)(v_{1},\dots,v_{n})=f(v_{1}\otimes\dots\otimes v_{n}),\
    v_{i}\in\Tangente[p]{M}\), con la notación del producto tensorial.
    
  \item \((2)\iff(3)\). Si \(f\colon\bigotimes_{n}\Tangente{M}\to\RealSet\)
    entonces
    \(\phi_{f}\colon\mathbb{X}(M)\times\dots\times\mathbb{X}(M)\to\mathcal{F}(M)\)
    está dada por \(\phi_{f}(X_{1},\dots,X_{n})(p)=f(X_{1}(p),\dots,X_{n}(p))\).
    Recíprocamente, si
    \(\phi\colon\mathbb{X}(M)\times\dots\times\mathbb{X}(M)\to\mathcal{F}(M)\)
    entonces \(f_{\phi}(v_{1}\otimes\dots\otimes
    v_{n})=\phi(X_{v_{1}},\dots,X_{v_{n}})\) donde \(X_{v_{i}}\) son campos
    cualesquiera con \(X_{v_{i}}(p)=v_{i}\). 
  \end{itemize}
\end{proof}

\section{Tensores}

Ahora podemos complicarlo más y mezclar el espacio tangente \(\Tangente{M}\) y
el cotangente \(\DualT{M}\) en las construcciones anteriores y hablar de
\textbf{tensores}.

\begin{definition}[name=tensores]
  Por un \emph{tensor r-covariante} se entenderá una r-forma sobre el espacio
  cotangente de \(M\), \(\bigotimes_{r}\DualT{M}\to\RealSet\) mientras que un
  \emph{tensor s-contravariante} es una s-forma del tangente,
  \(\bigotimes_{s}\Tangente{M}\to\RealSet\).

  \par
  
  Todavía más, si consideramos la variedad
  \((\bigotimes_{s}\Tangente{M})\otimes(\bigotimes_{r}\DualT{M})=\bigcup_{p\in
    M}(\bigotimes_{s}\Tangente[p]{M})\otimes(\bigotimes_{r}\DualT[p]{M})\), la
  aplicación diferenciable
  \((\bigotimes_{s}\Tangente{M})\otimes(\bigotimes_{r}\DualT{M})\to\RealSet\)
  que es lineal en cada \(p\in M\) será entonces un \emph{tensor mixto de tipo
    (s,r)}.
\end{definition}

\begin{remark}
  Si \(m=\dim{M}\) y tomamos una carta entorno de \(p\in M\)
  \((U,\varphi=(x_{1},\dots,x_{n}))\), con la notación
  \(\dif{x_{1}},\dots,\dif{x_{n}}\) y
  \(\pderiv{}{x_{1}},\dots,\pderiv{}{x_{n}}\) para la base canónica de
  \(\DualT[p]{M}\) y \(\Tangente[p]{M}\), tenemos que un tensor r-covariante es
  de la forma
  \[\sum_{(i_{1},\dots,i_{r})}g_{i_{1},\dots,i_{r}}\dif{x_{i_{1}}}\otimes\dots\otimes
  \dif{x_{i_{r}}},\ (i_{1},\dots,i_{r})\in\{1,\dots,m\}^{r}\] y análogamente un tensor
  s-contravariante se escribe como
  \[\sum_{(j_{1},\dots,j_{s})}f_{j_{1},\dots,j_{s}}\pderiv{}{x_{j_{1}}}\otimes\dots
  \otimes\pderiv{}{x_{j_{s}}},\ (j_{1},\dots,j_{s})\in\{1,\dots,m\}^{s}.\]

  Para un tensor mixto (s,r) tenemos
  \[\sum_{(i_{1},\dots,i_{r}),(j_{1},\dots,j_{s})}h_{i_{1},ots,i_{r}}^{j_{1},\dots,j_{s}}
  \pderiv{}{x_{j_{1}}}\otimes\dots\otimes\pderiv{}{x_{j_{s}}}\otimes
  \dif{x_{i_{1}}}\otimes\dots\otimes\dif{x_{i_{r}}}.\]

  A las funciones \(f,g,h\colon\varphi(U)\to\RealSet\) se las denomina
  \emph{componentes} del correspondiente tensor en la carta \((U,\varphi)\). 
\end{remark}

\section{Formas Simétricas y Alternadas}

Existen 2 clases de formas sobre un espacio vectorial \(V\) de especial interés:
las \textbf{formas simétricas y las antisimétricas o alternadas}.

\begin{definition}[name=forma simétrica]
  Una r-forma \(\alpha\colon V\times\dots\times V\to\RealSet\) se dice
  \emph{simétrica} si
  \[\alpha(v_{1},\dots,v_{i},\dots,v_{j},\dots,v_{r})=
  \alpha(v_{1},\dots,v_{j},\dots,v_{i},\dots,v_{r})\] para todo \(1\leq i\leq
  j\leq r\). 
\end{definition}

\begin{definition}[name=forma alternada]
  Una r-forma \(\alpha\colon V\times\dots\times V\to\RealSet\) se dice
  \emph{antisimétrica o alternada} si
  \[\alpha(v_{1},\dots,v_{i},\dots,v_{j},\dots,v_{r})= 
  -\alpha(v_{1},\dots,v_{j},\dots,v_{i},\dots,v_{r})\] para todo \(1\leq i\leq
  j\leq r\).  
\end{definition}

\begin{remark}
  Las formas alternadas también son llamadas \emph{exteriores} en algunas
  referencias. Incluso es bastante habitual que se reserve el nombre de
  ''forma'' exclusivamente para las alternadas, dejando aplicación multilineal
  para lo que aquí llamamos formas en general.
\end{remark}

\begin{example}
  El producto escalar de \(\RealSet^{n}\) es una 2-forma simétrica.
\end{example}

\begin{remark}\label{rem:inv-trasp}
  \begin{enumerate}
  \item Obsérvese que para una r-forma alternada \(\alpha\) sobre \(V\) se tiene
    que si \(v_{i}=v_{j},\ i\neq j\), entonces claramente
    \(\alpha(v_{1},\dots,v_{r})=0\) en virtud de su propia definición.
  \item Más aún, como toda permutación es producto de trasposiciones entonces para
    toda r-forma alternada \(\alpha\) y toda permutación \(\sigma\) de
    \(\{1,\dots,r\}\), tenemos
    \[\alpha(v_{\sigma(1)},\dots,v_{\sigma(r)})=\text{signo}(\sigma)
    \alpha(v_{1},\dots,v_{r}),\] 
    donde signo\((\sigma)\) vale \(1\) si \(\sigma\) se descompone como un
    número par de trasposiciones y \(-1\) si el número es impar.
  \item Por otro lado, si \(\alpha\) es simétrica entonces
    \[\alpha(v_{\sigma(1)},\dots,v_{\sigma(n)})=\alpha(v_{1},\dots,v_{n})\] pues
    \(\alpha\) es invariante por trasposiciones.
  \end{enumerate}
\end{remark}

Ahora se plantea el problema de buscar un ''producto'' de \(V\) adecuado
para las formas alternadas (simétricas, respectivamente).

\section{Producto Exterior y simétrico}
\begin{definition}[name= producto exterior]
El \emph{r-producto  exterior (o alternado)} de \( V \), \( \Lambda^rV \), es el expacio vectorial obtenido al hacer cociente del producto tensorial \( \otimes_rV=\underbracket{V\otimes \ldots \otimes V}_{r} \) por el subespacio generado por los productos \( v_1\otimes\ldots\otimes v_r\) con \( v_i= v_j \) para algún \(  i\neq j\).
\end{definition}

La clase de \( v_1\otimes\ldots\otimes v_r \) se denota \( v_1\wedge\ldots\wedge v_r \) y se llama \emph{producto exterior (o alternado)} de los vectores \( v_1,\ldots,v_r \).

De igual forma definimos el producto para las formas alternadas simétricas como sigue:

\begin{definition}[name=producto simétrico]
El \emph{r-producto simétrico} de \( V \), \( S^rV \), es el cociente de \( \otimes_rV \) por el subespacio generado por las diferencias
\[
v_1\otimes \ldots\otimes v_i\otimes \ldots v_j\otimes \ldots v_r-
v_1\otimes \ldots\otimes v_j\otimes \ldots v_i\otimes \ldots v_r
\]
para too \( i\neq j \).
\end{definition}

A la clase resultante \( \alpha_{1}\otimes\alpha_{r} \) se le denota \( \alpha_{1}\ldots \alpha_{r}\) y se llama \emph{producto simétrico} de los vectores \(\alpha_{1},\ldots ,\alpha_{r}  \).

Se tienen así aplicaciones cociente
\[
\pi_a\colon \otimes_r V\to \Lambda^rV\quad \pi_S\colon \otimes_rV\to S^rV
\]

Sean \( Alt_r(V) \) y \( Sim_r(V) \) los subespacios de \( Multi_r(V) \) formados por las r-formas alternas y simétricas, respectivamente. Vamos a construir de manera similar a la anterior aplicación lineal
\[
Alt\colon Multi_r(V)\to Alt_r(V)\quad Sim\colon Multi_r(V)\to Sim_r(V)
\]

Para ello observamos que si \( \beta\colon V\times V\to \RealSet \) es cualquier r-forma y \( \sigma \) es una permutación de \( \{1,\ldots,r\} \), tenemos entonce una nueva r-forma \( ^\sigma\beta\colon V\times\ldots V\to \RealSet \) definida como
\[
^\sigma\beta(v_1,\ldots,v_r)=\beta(v_{\sigma(1)},\ldots,v_{\sigma(r)})
\]

\begin{note}
Si \( \tau \) es otra permutación entonces para la composición \( \tau\sigma \) se tiene \( ^{\tau\sigma}\beta=^\tau(^\sigma\beta) \)
\end{note}

\section{Función simétrica y alternada}

\begin{definition}
Dada la forma \( \beta \) se define 
\[
Sim(\beta)=\frac{1}{r!}\sum_{\sigma\in G_r} ^\sigma\beta
\]
donde \( G_r \) es el grupo de las permutaciones de \( r \) elementos
\end{definition}

Esta definición tiene consecuencias inmediatas que veremos en forma de proposición.

\begin{proposition}
\( Sim(\beta) \)es efectivamente una r-forma simétrica
\end{proposition}

\begin{proof}
Sea \( \tau \) trasposición de \( i \) por \( j \). Tenemos
\begin{align*}
Sim(\beta)(v_1\ldots v_i\ldots v_j\ldots v_r)&=\frac{1}{r!}\sum_{\sigma\in G_r}^{\sigma\tau}\beta (v_1\ldots v_j\ldots v_i\ldots v_r)\\
&=\frac{1}{r!}\sum_{\sigma'\in G_r}^{\sigma'}\beta(v_1\ldots v_j\ldots v_i\ldots v_r)\\
&=Sim(\beta)(v_1\ldots v_j\ldots v_i\ldots v_r)
\end{align*}
Donde hemos usado que todo \( \sigma'\in G_r \) se puede reescribir como \( \sigma \tau \) siendo \( \sigma \) único si ya hemos fijado \( \tau \).
\end{proof}

\begin{proposition}
Si \( \beta \) es simétrica entonces \( Sim(\beta)=\beta \)
\end{proposition}

\begin{proof}
Ya hemos visto en \ref{rem:inv-trasp} y por tanto
\begin{align*}
Sim(\beta)(v_1\ldots v_r)&=\frac{1}{r!}\sum_{\sigma\in G_r}^\sigma\beta(v_1\ldots v_r)\\
&=\frac{1}{r!}(r!)\beta(v_1\ldots v_r)\\
&=\beta(v_1\ldots v_r)
\end{align*}
\end{proof}

Este resultado deja ver que la aplicación \( Sim \) concuerda con la noción intuitiva de la misma.

De manera análoga se define una aplicación para una r-forma arbitraria
\begin{definition} 
Sea \( \beta\in Multi_r(V) \)
\[
Alt(\beta)=\frac{1}{r!}\sum_{\sigma\in G_r}Signo(\sigma) ^\sigma\beta
\]
\end{definition}

Y cumple igualmente  que \( Alt(\beta) \) es una r-forma

\begin{proposition}
\( Alt(\beta) \) es una r-forma
\end{proposition}

\begin{proof}
	Sea \( \tau \) trasposición de \( i \) por \( j \). Tenemos
\begin{align*}
Alt(\beta)(v_1\ldots v_i\ldots v_j\ldots v_r)&=\frac{1}{r!}\sum_{\sigma\in G_r}Signo(\sigma) ^\sigma\beta(v_1\ldots v_i\ldots v_j\ldots v_r)\\
&=\frac{1}{r!}\sum_{\sigma\in G_r}Signo(\sigma) \beta(v_{\sigma(1)}\ldots v_{\sigma(i)}\ldots v_{\sigma(j)}\ldots v_{\sigma(r)})\\
&=\frac{1}{r!}\sum_{\sigma\in G_r}Signo(\sigma) \beta(v_{\tau\sigma(1)}\ldots v_{\tau\sigma(j)}\ldots v_{\tau\sigma(i)}\ldots v_{\tau\sigma(r)})\\
&=\frac{1}{r!}\sum_{\sigma\in G_r}-Signo(\tau\sigma) ^{\tau\sigma}\beta(v_1\ldots v_j\ldots v_i\ldots v_r)\\
&=-\frac{1}{r!}\sum_{\sigma'\in G_r}Signo(\sigma') ^{\sigma'}\beta(v_1\ldots v_i\ldots v_j\ldots v_r)\\
&=-Alt(\beta)(v_1\ldots v_j\ldots v_i\ldots v_r)
\end{align*}
\end{proof}

\begin{proposition}
Si \( \beta\in Alt_r(V) \) entonces \( Alt(\beta)=\beta \)
\end{proposition}

\begin{proof}
Sabemos que \( ^\sigma\beta =signo(\sigma)\beta \) de acuerdo con \ref{rem:inv-trasp} y por tanto
\begin{align*}
Alt(\beta)&=\frac{1}{r!}\sum_{\sigma\in G_r}Signo(\sigma) ^\sigma\beta\\
&=\frac{1}{r!}(r!)\beta\\
&=\beta
\end{align*}
\end{proof}

Además tenemos un diagrama que conecta todos los conceptos vistos en la sección
\begin{figure}[h]
	\centering
	\begin{tikzcd}
		Alt_r(V) & Multi_r(V) \arrow[l, "Alt"'] \arrow[r ,"Sim"]& Sim_r(V)\\
		(\Lambda^r V)^* \arrow[r, "\pi_a^*"] \arrow[u, "\rho_a"]& (\otimes_r V)^* \arrow[u, "\rho", "\homeo"'] & (S^rV)^* \arrow[l, "\pi_s^*"'] \arrow[u ,"\rho_s"]
	\end{tikzcd}
	%\caption{Texto}
	\label{fig:iso-ext}
\end{figure}
donde \( \rho \) es la biyección que lleva la 1-forma \( \gamma\colon V_1\otimes\ldots \otimes V_r\to \RealSet \) en la r-forma \( \rho(\gamma)\colon V_1\times\ldots\times V_r\to \RealSet \) definida como
\[
\rho(\alpha)(v_1\ldots v_r)=\gamma(v_1\otimes \ldots \otimes v_r)
\]

Entonces \( \rho_s=Sim\circ \rho \circ \pi_s^* \) y \( \rho_a=Alt\circ \rho \circ \pi_a^* \) son también biyectivas.

\begin{proposition}
\( \rho_a \) lleva la 1-forma \( \gamma\colon \Lambda^rV\to \RealSet \) en la r-forma alternada
\[
\rho_a(\gamma)(v_1,\ldots,v_r)=\gamma(v_1\wedge\ldots\wedge v_r)
\]
\end{proposition}

\begin{proof}
Siguiendo la definición del diagrama anterior
\begin{align*}
\rho_s(\gamma)(v_1\ldots v_r)&=Alt\circ \rho_s\circ \pi_a^*(\gamma)\\
&=Alt(\rho(\gamma\circ \pi_a))(v_1\ldots v_r)\\
&=\frac{1}{r!}\sum_{\sigma\in G_r}Signo(\sigma) ^\sigma\rho(\gamma\circ \pi_a)(v_1\ldots v_r)\\
&=\frac{1}{r!}\sum_{\sigma\in G_r}Signo(\sigma) \rho(\gamma\circ \pi_a)(v_{\sigma(1)}\ldots v_{\sigma(r)})\\
&=\frac{1}{r!}\sum_{\sigma\in G_r}Signo(\sigma) \gamma\circ \pi_a(v_{\sigma(1)}\otimes\ldots\otimes v_{\sigma(r)})\\
&=\frac{1}{r!}\sum_{\sigma\in G_r}Signo(\sigma)Signo(\sigma) \gamma\circ \pi_a(v_{1}\otimes\ldots\otimes v_{r})\\
&=\frac{1}{r!}(r!\gamma(v_1\wedge\ldots \wedge v_r))\\
&=\gamma(v_1\wedge\ldots \wedge v_r)
\end{align*}
usando que \( v_{\sigma(1)}\otimes \ldots v_{\sigma(r)} \) y \( Signo(\sigma)(v_1\otimes\ldots \otimes v_r) \) representan la misma clase en \( \Lambda^rV \) como cociente de \( \otimes_rV \)
\end{proof}

De aquí se sigue inmediatamente que\( \rho_a \) es inyectiva. \\
Para la sobreyectividad, si \( \xi\colon V_1\times\ldots\times V_r\to \RealSet\) es una forma alternada, entonces \( \rho^{-1}(\xi)\colon V_1\otimes\ldots\otimes V_r\to \RealSet \) cumple
\begin{align*}
\rho^{-1}(\xi)(v_1\otimes\ldots\otimes v_i\otimes\ldots\otimes v_j\otimes\ldots\otimes v_r)&=\xi(v_1,\ldots,v_i,\ldots,v_j,\ldots,v_r)\\
&=-\xi (v_1,\ldots,v_j,\ldots,v_r)
\end{align*}
por lo que es compatible con el cociente que define \( \Lambda^rV \) e induce una 1-forma \( \tilde{\rho^{-1}(\xi)}\colon \Lambda^rV\to \RealSet \) con
\[
\pi_a^*(\tilde{\rho^{-1}(\xi)})=\tilde{\rho^{-1}(\xi)}\circ \pi =\rho^{-1}(\xi)
\]
Así pues \( \rho_a(\tilde{\rho^{-1}(\xi)})=\xi \)

\begin{remark}
Para \( \rho_s\colon (S^rV)^*\to Sim_r(V) \) tenemos análogamente que si \( \nu\in (S^rV)^* \) entonces
\[
\rho_s(\nu)(v_1,\ldots,v_r)=\nu(\underbracket{v_1v_2\cdots v_r}_{\text{prod simétrico}})
\]
\end{remark}

Hemos probado así los isomorfismos
\[
(\Lambda^rV)^*\homeo Alt_r(V),\quad (S^rV)^*\homeo Sim_r(V)
\]

\section{Dimensiones}
\begin{proposition}
Si \( \{e_i\} \) es una base de \( V \) entonces los productos exteriores \( e_{i_1}\wedge \ldots\wedge e_{i_r} \) con \( i_1<\ldots< i_r \) forman base de \( \Lambda^rV \). Además su dimensión es
\[
(\overset{n}{r})=\frac{n!}{r!(n-r)!}
\]
siendo \( n=dim(V) \)
\end{proposition}

\begin{proof}
Podemos comprobar que toda r-forma en \( Alt_r(V) \) queda determinada por sus valores en \( (e_{i_1}),\ldots,e_{i_r} \) con \( i_1<\ldots <i_r \). Por tanto su dimensión es el número de posibles combinaciones con los subíndices.
\end{proof}

Tenemos un resultado análogo para \( S^rV \)

\begin{proposition}
Una base de \( S^rV \) la forman los productos simétricos \( e_{i_1}\cdots e_{i_r} \) con \( i_1\leq \ldots\leq i_r \) y su dimensión es
\[
dimS^r(V)=(\overset{n+r-1}{r})=\frac{(n+r-1)!}{r!(n-1)!}
\]
\end{proposition}

\begin{proof}
La prueba se hará en clase de problemas.
\end{proof}

\begin{note}
Observese que para \( n=dim(V) \) tenemos \( Alt_nV\homeo\Lambda^nV\homeo \RealSet \) estando \( \Lambda^nV \) generado por \( e_1\wedge\ldots\wedge e_n \) siendo \( \{e_i\} \) una base de \( V \).
\end{note}

\begin{proposition}
La n-forma alternada se puede interpretar como un determinante
\end{proposition}

\begin{proof}
En efecto, si \( \mu \) es una n-forma alternada, entonces \( \mu(v_1,\ldots,v_n)=\lambda\mu(e_1,\ldots,e_n) \), donde \( \lambda \) es el determinante de la matriz \( n\times n \) \( A_r \) cuyos vectores columna son los \( v_i \).

Si \( v_j=\sum_{i=1}^{n}\lambda_j^ie_i \), entonces tomando \( \mu \) en \( Multi_n(V)\homeo (\otimes_nV)^* \), tenemos por la multilinealidad del producto tensorial
\begin{align*}
\mu(v_1\ldots v_n)&=\mu(\sum_{i=1}^n \lambda_1^ie_i\otimes\ldots \otimes\sum_{i=1}^n\lambda_n^i e_i)\\
&=\sum_{(i_1,\ldots,i_n)\in \{1,\ldots,n\}^n} \lambda_1^{i_1}\cdots\lambda_n^{i_n}\mu(e_{i_1}\otimes\ldots\otimes e_{i_n})\quad (2)
\end{align*}
Ahora bien, por se \( \mu \) alternada esta se anula cuando aparecen índices repetidos. Así pues \( i_1\ldots i_n\) debe ser una permutación de \( \{1,\ldots,n\} \) y
\begin{align*}
(2)&=\sum_{\sigma\in G_n}\lambda_1^{\sigma(1)}\cdots\lambda_n^{\sigma(n)}\mu(e_{\sigma(1)}\otimes\ldots\otimes e_{\sigma(n)})\\
&=\sum_{\sigma\in G_n}Signo(\sigma)\lambda_1^{\sigma(1)}\cdots\lambda_n^{\sigma(n)}\mu(e_1\otimes\ldots\otimes e_n)\\
&=\lambda\mu(e_1\otimes\ldots\otimes e_n)\\
&=\lambda\mu(e_1\wedge\ldots\wedge e_n)
\end{align*}
donde \( \lambda=\sum_{\sigma\in G_n}Signo(\sigma)\lambda_1^{\sigma(1)}\ldots\lambda_n^{\sigma(n)}=det(A_v) \).
\end{proof}

\section{Producto simétrico y exterior entre formas}
Dadas una r-forma \( \alpha\colon\otimes_rV\to \RealSet \) y una s-forma \( \beta\colon\otimes_sV\to \RealSet \) habíamos definido la (r+s)-forma \( \alpha\otimes\beta\colon\otimes_{r+s}V\to \RealSet \) por
\[
\alpha\otimes\beta(v_1\otimes\ldots\otimes v_r\otimes v_{r+1}\otimes\ldots\otimes v_{r+s})=\alpha(v_1\ldots v_r)\cdot\beta(v_{r+1}\ldots v_{r+s})
\]

Si \( \alpha \)y \( \beta \) son simétricos entonces \( \alpha\otimes\beta \) no tiene que ser simétrico. Lo mismo ocurre si son alternados. Es por ellos que buscamos definir un nuevo producto que sea interno dentro de los simétricos y alternados.

\begin{definition}
Definimos el \emph{producto simétrico} de la r-forma simétrica \( \alpha \) con la s-forma simétrica \( \beta \) como la "simetrización" del producto anterior
\[
\alpha\cdot\beta=Sim(\alpha\otimes\beta)
\]
\end{definition}

Si lo vemos aplicado a un elemento genérico resulta
\begin{align*}
&\alpha\cdot\beta(v_1,\ldots,v_r,v_{r+1},\ldots,v_{r+s})=\\
&\frac{1}{(r+s)!}\sum_{\sigma\in G_{r+s}}\alpha(v_{\sigma(1)}\ldots v_{\sidebarhsep(n)})\beta(v_{\sigma(r+1)}\ldots v_{\sigma(r+s)})
\end{align*}

\begin{definition}
Definimos el \emph{producto exterior (o alternado)} de la r-forma alternada \( \alpha \) con la s-forma alternada \( \beta \) como
\[
\alpha\wedge\beta=Alt(\alpha\otimes\beta)
\]
es decir,
\begin{align*}
&\alpha\wedge\beta(v_1,\ldots,v_r,v_{r+1},\ldots,v_{r+s})=\\
&\frac{1}{(r+s)!}\sum_{\sigma\in G_{r+s}}Signo(\sigma)\alpha(v_1\ldots v_r)\cdot\beta(v_{r+1}\ldots v_{r+s})\\
\end{align*}
\end{definition}

Veamos ahora una serie de propiedades acerca de estos nuevos conceptos.

\begin{proposition}\label{prop:ext-proper}
El producto exterior de formas alternadas cumple las siguientes propiedades:
\begin{enumerate}
\item Bilineal: \( (a\alpha+b\beta)\wedge \eta=a(\alpha\wedge\eta)+b(\beta\wedge\eta) \)
\item Asociativo: \( (\alpha\wedge\beta)\wedge \gamma=\alpha\wedge(\beta\wedge\gamma) \)
\item Anticonmutativo: \( \omega\wedge\eta=(-1)^{rs}\eta\wedge\omega \) si \( \omega\in \Lambda^r(V) \) y \( \eta\in \Lambda^s(V) \). En particular, \( \omega\wedge\omega=0 \) si r es impar.
\end{enumerate}
\end{proposition}

\begin{proof}
\begin{enumerate}
\item Esta propiedad es inmediata a partir de la definición.
\item Lo probaremos en el siguiente lema \ref{lem:transi}
\item Sabiendo que cada término de
\begin{align*}
&Alt(\omega\otimes\eta)(v_1,\ldots,v_r,v_{r+1},\ldots,v_{r+s})=\\
&\frac{1}{(r+s)!}\sum_{\sigma\in G_{r+s}}Signo(\sigma)\omega(v_{\sigma(1)},\ldots,\omega_{\sigma(r)})\eta(v_{\sigma(r+1)},\ldots,v_{\sigma(r+s)})
\end{align*}
aparece en
\begin{align*}
&Alt(\eta\otimes\omega)(v_1,\ldots,v_r,v_{r+1},\ldots,v_{r+s})=\\
&\frac{1}{(r+s)!}\sum_{\sigma'\in G_{r+s}}Signo(\sigma')\eta(v_{\sigma'(1)},\ldots,\omega_{\sigma'(r)})\omega(v_{\sigma'(r+1)},\ldots,v_{\sigma'(r+s)})
\end{align*}
En efecto, dada \( \sigma \) su \( \sigma' \) asociada está definida por
\(
\sigma'(1)=\sigma(r+1),\ldots,\sigma'(s)=\sigma(r+s),\sigma'(s+1)=\sigma(1),\ldots,\sigma'(s+r)=\sigma(r)
\)
Falta por ver la relación entre los signos. Siguiendo la regla anterior es fácil ver que para pasar de \( \sigma'(1) \) a \( \sigma(1) \) necesitamos trasponer r posiciones y lo mismo ocurre para cualquier otro índice.

Así pues \( signo(\sigma)=signo(\sigma')(-1)^{rs} \).
\end{enumerate}
\end{proof}

\begin{lemma}\label{lem:transi}
	\begin{enumerate}
\item Supongamos que la r-forma \( \alpha \) cumple \( Alt(\alpha)=0 \). Entonces para toda s-forma \( \beta \) se tiene \( Alt(\alpha\otimes\beta)=Alt(\beta\otimes\alpha)=0 \).
\item \( Alt(Alt(\omega\otimes\eta)\otimes\theta)=Alt(\omega\otimes\eta\otimes\theta)=Alt(\omega\otimes Alt(\eta\otimes \theta)) \).
\end{enumerate}
\end{lemma}

\begin{proof}
\begin{enumerate}
\item Desarrollamos uno de los dos dado que son análogos.
\begin{align*}
&Alt(\alpha\otimes\beta)(v_1\ldots v_{r+s})=\\
&=\frac{1}{(r+s)!}\sum_{\sigma\in G_{r+s}}Signo(\sigma)\alpha(v_{\sigma(1)},\ldots,\omega_{\sigma(r)})\beta(v_{\sigma(r+1)},\ldots,v_{\sigma(r+s)})
\end{align*}
Para probar que esta suma es nula se descompondrá \( G_{r+s} \) en subconjuntos disjuntos tales que sumas extendidas a esos subconjuntos serán todas nulas.\\
Empezamos tomando \( G\in G_{r+s} \) formado por las permutaciones que dejen fijo \( r+1,\ldots,r+s \). Entonces si \( \sigma' \) es la restricción de \( \sigma \) a \( \{1,\ldots,r\} \), tenemos que \( \sigma\mapsto \sigma' \) da un isomorfismo \( G\homeo G_r \) y además \( signo(\sigma)=signo(\sigma') \). Entonces
\begin{align*}
&\sum_{\sigma\in G}Signo(\sigma)\alpha(v_{\sigma(1)},\ldots,\omega_{\sigma(r)})\beta(v_{\sigma(r+1)},\ldots,v_{\sigma(r+s)})=\\
&=\sum_{\sigma\in G_{r+s}}Signo(\sigma)\alpha(v_{\sigma(1)},\ldots,\omega_{\sigma(r)})\beta(v_{r+1},\ldots,v_{r+s})=\\
&=[\sum_{\sigma'\in G_{r+s}}Signo(\sigma')\alpha(v_{\sigma'(1)},\ldots,\omega_{\sigma'(r)})]\beta(v_{r+1},\ldots,v_{r+s})=\\
&=r!Alt(\alpha)\beta(v_{r+1}\ldots v_{r+s})=0
\end{align*}
Sea \( \sigma_1\neq G \). Entonces \( G\sigma_1=\{\sigma\sigma_1\colon\sigma\in G\} \) es disjunto con \( G \) pues si \( \mu\in G\sigma_1\cap G\) entonces \( \mu=\sigma\sigma_1 \) con \( \mu,\sigma\in G \) y se cumpliría \( \sigma_1=\sigma^{-1}\mu\in G \) que es contradicción.\\
El sumando generado por \( G\sigma_1 \) es
\begin{align*}
&\sum_{\sigma_1}=\sum_{\mu\in G\sim_1}Signo(\mu)\alpha(v_{\mu(1)}\ldots v_{\mu(r)})\beta(v_{\mu(r+1)}\ldots v_{\mu(r+s)})=\\
&=\sum_{\theta\in G}Signo(\sigma\sigma_1)\alpha(v_{\sigma\sigma_1(1)}\ldots v_{\sigma\sigma_1(r)})\beta(v_{\sigma\sigma_1(r+1)}\ldots v_{\sigma\sigma_1(r+s)})\quad (\label{proof:lem-transi})
\end{align*}
Tomando \( (w_1\ldots w_{k+l})=(v_{\sigma_1(1)}\ldots v_{\sigma_1(r+s)}) \), tenemos que \( w_{\sigma(i)}=v_{\sigma\sigma_1(i)} \) y por tanto
\begin{align*}
(\ref{proof:lem-transi} )&=Signo(\sigma_1)\sum_{\sigma\in G}Signo(\sigma)\alpha(w_{\sigma(1)}\ldots w_{\sigma(r)})\beta(w_{\sigma(r+1)}\ldots w_{\sigma(r+s)})\\
&=Signo(\sigma_1)(r!Alt(\alpha)(w_1\ldots w_r))\beta(w_{r+1}\ldots w_{r+s})\\
&=0
\end{align*}
Si tomamos \( \sigma_2\notin G\cup G\sigma_1 \), tenemos \( G\cap G\sigma_2=\emptyset \) y \( G\sigma_1\cap G\sigma_2=\emptyset \).\\
Repitiendo el proceso anterior llegamos a \( \sum_{\sigma_2}=0 \).

Por la finitud de \( G_{r+s} \) se llegará a cubrir \( G_{r+s} \) con conjuntos disjuntos de la forma \( G, G_{\sigma_1},\ldots,G_{\sigma_k} \) y cada uno de ellos no aporta nada a la suma total, que será por tanto nula.
\item Observamps que por ser \( Alt(\alpha) \) una forma alternada \( Alt(Alt(\alpha))=Alt(\alpha) \). Por tanto
\begin{align*}
\alpha&=Alt(Alt(\eta\otimes\theta)-\eta\otimes\theta)\\
&=Alt(\eta\otimes\theta)-Alt(\eta\otimes\theta)\\
&=0
\end{align*}
Entonces por (1), linealidad y la transitividad del producto tensorial
\[
0=Alt(\omega\otimes\alpha)=Alt(\omega\otimes Alt(\eta\otimes\theta))-Alt(\omega\otimes\eta\otimes\theta)
\]
Por tanto \( Alt(\omega\otimes Alt(\eta\otimes\theta))=Alt(\omega\otimes\eta\otimes\theta) \).

De manera análoga se prueba \( Alt(Alt(\omega\otimes Alt(\eta\otimes\theta)))=Alt(\omega\otimes\eta\otimes\theta) \)
\end{enumerate}
\end{proof}

\begin{note}
Usando la segunda parte del lema \ref{lem:transi} tenemos
\[
(\alpha\wedge\beta)\wedge\gamma=Alt(Alt(\alpha\otimes\beta)\otimes\gamma)=
Alt(\alpha\otimes Alt(\beta\otimes\gamma))=\alpha\wedge(\beta\wedge\gamma)
\]
lo que probaría (2) \ref{prop:ext-proper}
\end{note}

La siguiente proposición nos dice que las formas alternadas quedan determinadas por las 1-formas t el producto exterior.

\begin{proposition}
Si \( \{e_i\} \) es una base de \( V \), entonces la r-forma alternada \( r!(e_{i_1}^*\wedge\ldots\wedge e_{i_r}^*) \) con \( 1\leq i_1< \ldots<i_r\leq n  \) se corresponde con el del elemento básico \( e_{i_1}\wedge\ldots \wedge e_{i_r}\in \Lambda^r(V) \)  por el isomorfismo \( \rho_a\colon(\Lambda^rV)^*\to Alt_r(V)  \) que vimos en \ref{fig:iso-ext}.

En particular al recorrer  \( 1\leq i_1< \ldots<i_r\leq n  \) todas las ordenaciones, el conjunto \( \{e_{i_1}^*\wedge\ldots\wedge e_{i_r}^*\}\) forma una base de \( alt_r(V) \).
\end{proposition}

\begin{proof}
Lo haremos por inducción.\\
Para r=1 es obvio.\\
Supongamos el resultado cierto para r-1 y veamos el caso r. Por definición
\begin{align*}
&\rho_a^{-1}(e_{i_1}^*\wedge\ldots e_{i_r}^*)(e_{j_1}\wedge\ldots\wedge e_{j_r})=\\
&=(e_{i_1}^*\wedge\ldots e_{i_r}^*)(e_{j_1},\ldots, e_{j_r})=\\
&=\frac{1}{r!}\sum_{\sigma\in G_r}Signo(\sigma)e_{i_1}^*(e_{j_{\sigma(1)}})(e_{i_2}^*\wedge\ldots e_{i_r}^*)(e_{j_{\sigma(2)}},\ldots, e_{j_{\sigma(r)}})\quad (\label{proof:prop-alt})
\end{align*}
Aquí usamos la transitividad del producto exterior de formas. Ahora \( e_{i_1}^*(e_{j_1})=1 \) si para todo \( \sigma\in G_r \) se tiene \( j_{\sigma(1)}=i_1 \).

Por tanto existe un \( 1\leq t\leq r \) con \( j_t=i_1 \) y tenemos
\begin{align*}
&(\ref{proof:prop-alt})=\frac{1}{r!}\sum_{\sigma\in G_r \sigma(1)=t}Signo(\sigma)(e_{i_2}^*\wedge\ldots e_{i_r}^*)(e_{j_{\sigma(2)}},\ldots, e_{j_{\sigma(r)}})=\\
&=\frac{1}{r!}\sum_{\sigma'\in G_r}Signo(\sigma')(e_{i_2}^*\wedge\ldots e_{i_r}^*)(e_{j_{\sigma'(2)}},\ldots, e_{j_{\sigma'(r)}})\quad \label{proof:prop-alt-2}
\end{align*}
Aquí \( \sigma' \) es la restricción de \( \sigma \) a \( \{1,\ldots,t-1,t+1,\ldots,r\} \). Observese que \( Signo(\sigma')=Signo(\sigma) \).

Por hipótesis de inducción \( (r-1)!((e_{i_2}^*\wedge\ldots e_{i_r}^*) \) corresponde al dual de \( (e_{i_2}\wedge\ldots e_{i_r} \), por lo que necesariamente \( \{j_1,\ldots,j_{t-1},j_{t+1},\ldots,j_r\}=\{i_2,\ldots,i_r\} \) y además
\[
(r-1)!((e_{i_2}^*\wedge\ldots e_{i_r}^*)(e_{j_{\sigma'(2)}},\ldots, e_{j_{\sigma'(r)}})=Signo(\sigma')
\]
por lo que
\begin{align*}
(\ref{proof:prop-alt-2})&=\frac{1}{r!}\frac{1}{(r-1)!}\sum_{\sigma'\in G_{r+-1}}(Signo(\sigma'))^2\\
&=\frac{1}{r!}\frac{1}{(r-1)!}(r-1)!\\
&=\frac{1}{r!}
\end{align*}
De donde se sigue el resultado.
\end{proof}

\end{document}

%%% Local Variables:
%%% TeX-master: "../VD"
%%% End: